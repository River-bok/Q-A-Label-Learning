{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917f6d0a",
   "metadata": {},
   "source": [
    "# model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9684d738",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class linear_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(linear_model, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, self.num_flat_features(x))\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "class mlp_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(mlp_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, self.num_flat_features(x))\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb91b5",
   "metadata": {},
   "source": [
    "# data algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48508db6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vc/5k5wvfn57lx1n7gwss_y_ksc0000gn/T/ipykernel_46825/1385116394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def accuracy_check(loader, model, device):\n",
    "    with torch.no_grad():\n",
    "        total, num_samples = 0, 0\n",
    "        for images, labels in loader:\n",
    "            labels, images = labels.to(device), images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += (predicted == labels).sum().item()\n",
    "            num_samples += labels.size(0)\n",
    "    return total / num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ace31",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.models as models\n",
    "from scipy.special import comb\n",
    "\n",
    "def generate_uniform_comp_labels(dataname, labels):\n",
    "    if torch.min(labels) > 1:\n",
    "        raise RuntimeError('testError')\n",
    "    elif torch.min(labels) == 1:\n",
    "        labels = labels - 1\n",
    "\n",
    "    K = torch.max(labels) - torch.min(labels) + 1\n",
    "    n = labels.shape[0]\n",
    "    cardinality = 2**K - 2\n",
    "    number = torch.tensor([comb(K, i+1) for i in range(K-1)]) # 0 to K-2, convert list to tensor\n",
    "    frequency_dis = number / cardinality\n",
    "    prob_dis = torch.zeros(K-1) # tensor of K-1\n",
    "    for i in range(K-1):\n",
    "        if i == 0:\n",
    "            prob_dis[i] = frequency_dis[i]\n",
    "        else:\n",
    "            prob_dis[i] = frequency_dis[i]+prob_dis[i-1]\n",
    "\n",
    "    random_n = torch.from_numpy(np.random.uniform(0, 1, n)).float() # tensor: n\n",
    "    mask_n = torch.ones(n) # n is the number of train_data\n",
    "    partialY = torch.ones(n, K)\n",
    "    temp_num_comp_train_labels = 0 # save temp number of comp train_labels\n",
    "    \n",
    "    for j in range(n): # for each instance\n",
    "        if j % 1000 == 0:\n",
    "            print(\"current index:\", j)\n",
    "        for jj in range(K-1): # 0 to K-2\n",
    "            if random_n[j] <= prob_dis[jj] and mask_n[j] == 1:\n",
    "                temp_num_comp_train_labels = jj+1 # decide the number of complementary train_labels\n",
    "                mask_n[j] = 0\n",
    "\n",
    "        candidates = torch.from_numpy(np.random.permutation(K.item())) # because K is tensor type\n",
    "        candidates = candidates[candidates!=labels[j]]\n",
    "        temp_comp_train_labels = candidates[:temp_num_comp_train_labels]\n",
    "        \n",
    "        for kk in range(len(temp_comp_train_labels)):\n",
    "            partialY[j, temp_comp_train_labels[kk]] = 0 # fulfill the partial label matrix\n",
    "    return partialY\n",
    "\n",
    "def generate_multi_comp_labels(data, labels, s):\n",
    "    k = torch.max(labels) + 1\n",
    "    n = labels.shape[0]\n",
    "    index_ins = torch.arange(n) # torch type\n",
    "    realY = torch.zeros(n, k)\n",
    "    realY[index_ins, labels] = 1\n",
    "    partialY = torch.ones(n, k)\n",
    "    \n",
    "    labels_hat = labels.clone().numpy()\n",
    "    candidates = np.repeat(np.arange(k).reshape(1, k), len(labels_hat), 0) # candidate labels without true class\n",
    "    mask = np.ones((len(labels_hat), k), dtype=bool)\n",
    "    for i in range(s):\n",
    "        mask[np.arange(n), labels_hat] = False\n",
    "        candidates_ = candidates[mask].reshape(n, k-1-i)\n",
    "        idx = np.random.randint(0, k-1-i, n)\n",
    "        comp_labels = candidates_[np.arange(n), np.array(idx)]\n",
    "        partialY[index_ins, torch.from_numpy(comp_labels)] = 0\n",
    "        if i == 0:\n",
    "            complementary_labels = torch.from_numpy(comp_labels)\n",
    "            multiple_data = data\n",
    "        else:\n",
    "            complementary_labels = torch.cat((complementary_labels, torch.from_numpy(comp_labels)), dim=0)\n",
    "            multiple_data = torch.cat((multiple_data, data), dim = 0)\n",
    "        labels_hat = comp_labels\n",
    "    return partialY\n",
    "        \n",
    "def class_prior(complementary_labels):\n",
    "    return np.bincount(complementary_labels) / len(complementary_labels)\n",
    "\n",
    "def prepare_mnist_data(batch_size):\n",
    "    ordinary_train_dataset = dsets.MNIST(root='./data/mnist', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = dsets.MNIST(root='./data/mnist', train=False, transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, num_classes\n",
    "\n",
    "def prepare_kmnist_data(batch_size):\n",
    "    ordinary_train_dataset = dsets.KMNIST(root='./data/KMNIST', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = dsets.KMNIST(root='./data/KMNIST', train=False, transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, num_classes\n",
    "\n",
    "def prepare_fashion_data(batch_size):\n",
    "    ordinary_train_dataset = dsets.FashionMNIST(root='./data/FashionMnist', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = dsets.FashionMNIST(root='./data/FashionMnist', train=False, transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, num_classes\n",
    "\n",
    "def prepare_cifar10_data(batch_size):\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), # transforms.RandomHorizontalFlip(), transforms.RandomCrop(32,4),\n",
    "         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    ordinary_train_dataset = dsets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_dataset = dsets.CIFAR10(root='./data', train=False, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    return full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, 10\n",
    "\n",
    "def prepare_train_loaders_for_multi_comp_labels(full_train_loader, batch_size, s):\n",
    "    for i, (data, labels) in enumerate(full_train_loader):\n",
    "        K = torch.max(labels)+1 # K is number of classes, full_train_loader is full batch\n",
    "    partialY = generate_multi_comp_labels(data, labels, s)\n",
    "    partial_matrix_dataset = torch.utils.data.TensorDataset(data, partialY.float())\n",
    "    partial_matrix_train_loader = torch.utils.data.DataLoader(dataset=partial_matrix_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dim = int(data.reshape(-1).shape[0]/data.shape[0])\n",
    "    return partial_matrix_train_loader, dim\n",
    "\n",
    "def prepare_train_loaders_for_uniform_comp_labels(dataname, full_train_loader, batch_size):\n",
    "    for i, (data, labels) in enumerate(full_train_loader):\n",
    "        K = torch.max(labels)+1 # K is number of classes, full_train_loader is full batch\n",
    "    partialY = generate_uniform_comp_labels(dataname, labels)\n",
    "    partial_matrix_dataset = torch.utils.data.TensorDataset(data, partialY.float())\n",
    "    partial_matrix_train_loader = torch.utils.data.DataLoader(dataset=partial_matrix_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dim = int(data.reshape(-1).shape[0]/data.shape[0])\n",
    "    return partial_matrix_train_loader, dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23791c",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def mae_loss(outputs, partialY):\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    loss_fn = nn.L1Loss(reduction='none')\n",
    "    loss_matrix = loss_fn(sm_outputs, partialY.float())\n",
    "    sample_loss = loss_matrix.sum(dim=-1)\n",
    "    return sample_loss\n",
    "    \n",
    "def mse_loss(outputs, Y):\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    loss_matrix = loss_fn(sm_outputs, Y.float())\n",
    "    sample_loss = loss_matrix.sum(dim=-1)\n",
    "    return sample_loss\n",
    "\n",
    "def gce_loss(outputs, Y):\n",
    "    q = 0.7\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    pow_outputs = torch.pow(sm_outputs, q)\n",
    "    sample_loss = (1-(pow_outputs*Y).sum(dim=1))/q # n\n",
    "    return sample_loss\n",
    "\n",
    "def phuber_ce_loss(outputs, Y):\n",
    "    trunc_point = 0.1\n",
    "    n = Y.shape[0]\n",
    "    soft_max = nn.Softmax(dim=1)\n",
    "    sm_outputs = soft_max(outputs)\n",
    "    final_outputs = sm_outputs * Y\n",
    "    final_confidence = final_outputs.sum(dim=1)\n",
    "   \n",
    "    ce_index = (final_confidence > trunc_point)\n",
    "    sample_loss = torch.zeros(n).to(device)\n",
    "\n",
    "    if ce_index.sum() > 0:\n",
    "        ce_outputs = outputs[ce_index,:]\n",
    "        logsm = nn.LogSoftmax(dim=-1)\n",
    "        logsm_outputs = logsm(ce_outputs)\n",
    "        final_ce_outputs = logsm_outputs * Y[ce_index,:]\n",
    "        sample_loss[ce_index] = - final_ce_outputs.sum(dim=-1)\n",
    "\n",
    "    linear_index = (final_confidence <= trunc_point)\n",
    "\n",
    "    if linear_index.sum() > 0:\n",
    "        sample_loss[linear_index] = -math.log(trunc_point) + (-1/trunc_point)*final_confidence[linear_index] + 1\n",
    "\n",
    "    return sample_loss\n",
    "\n",
    "def ce_loss(outputs, Y):\n",
    "    logsm = nn.LogSoftmax(dim=1)\n",
    "    logsm_outputs = logsm(outputs)\n",
    "    final_outputs = logsm_outputs * Y\n",
    "    sample_loss = - final_outputs.sum(dim=1)\n",
    "    return sample_loss\n",
    "\n",
    "def unbiased_estimator(loss_fn, outputs, partialY, device):\n",
    "    n, k = partialY.shape[0], partialY.shape[1]\n",
    "    comp_num = k - partialY.sum(dim=1)\n",
    "    temp_loss = torch.zeros(n, k).to(device)\n",
    "    for i in range(k):\n",
    "        tempY = torch.zeros(n, k).to(device)\n",
    "        tempY[:, i] = 1.0\n",
    "        temp_loss[:, i] = loss_fn(outputs, tempY)\n",
    "\n",
    "    candidate_loss = (temp_loss * partialY).sum(dim=1)\n",
    "    noncandidate_loss = (temp_loss * (1-partialY)).sum(dim=1)\n",
    "    total_loss = candidate_loss - (k-comp_num-1.0)/comp_num * noncandidate_loss\n",
    "    average_loss = total_loss.mean()\n",
    "    return average_loss\n",
    "\n",
    "def log_loss(outputs, partialY):\n",
    "    k = partialY.shape[1]\n",
    "    can_num = partialY.sum(dim=1).float() # n\n",
    "    \n",
    "    soft_max = nn.Softmax(dim=1)\n",
    "    sm_outputs = soft_max(outputs)\n",
    "    final_outputs = sm_outputs * partialY\n",
    "    \n",
    "    average_loss = - ((k-1)/(k-can_num) * torch.log(final_outputs.sum(dim=1))).mean()\n",
    "    return average_loss\n",
    "\n",
    "def exp_loss(outputs, partialY):\n",
    "    k = partialY.shape[1]\n",
    "    can_num = partialY.sum(dim=1).float() # n\n",
    "    \n",
    "    soft_max = nn.Softmax(dim=1)\n",
    "    sm_outputs = soft_max(outputs)\n",
    "    final_outputs = sm_outputs * partialY\n",
    "\n",
    "    average_loss = ((k-1)/(k-can_num) * torch.exp(-final_outputs.sum(dim=1))).mean()\n",
    "    return average_loss    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d87bd",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.manual_seed(0); torch.cuda.manual_seed_all(0)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#if args.ds == 'mnist':\n",
    "#    full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, K = prepare_mnist_data(batch_size=args.bs)\n",
    "#elif args.ds == 'kmnist':\n",
    "#    full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, K = prepare_kmnist_data(batch_size=args.bs)\n",
    "#elif args.ds == 'fashion':\n",
    "#   full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, K = prepare_fashion_data(batch_size=args.bs)\n",
    "#elif args.ds == 'cifar10':\n",
    "full_train_loader, train_loader, test_loader, ordinary_train_dataset, test_dataset, K = prepare_cifar10_data(batch_size=args.bs)\n",
    "\n",
    "\n",
    "partial_matrix_train_loader, dim = prepare_train_loaders_for_multi_comp_labels(full_train_loader=full_train_loader, batch_size=args.bs, s=5)\n",
    "\n",
    "loss_fn = mae_loss\n",
    "\n",
    "model = mlp_model(input_dim=dim, hidden_dim=500, output_dim=K)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.wd)\n",
    "\n",
    "train_accuracy = accuracy_check(loader=train_loader, model=model, device=device)\n",
    "test_accuracy = accuracy_check(loader=test_loader, model=model, device=device)\n",
    "\n",
    "print('Epoch: 0. Tr Acc: {}. Te Acc: {}'.format(train_accuracy, test_accuracy))\n",
    "\n",
    "test_acc_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "for epoch in range(args.ep):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(partial_matrix_train_loader):\n",
    "        X, partialY = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        \n",
    "        if args.lo == 'exp' or args.lo == 'log':\n",
    "            average_loss = loss_fn(outputs, partialY.float())\n",
    "        else:\n",
    "            average_loss = unbiased_estimator(loss_fn, outputs, partialY.float(), device)\n",
    "        average_loss.backward()\n",
    "        optimizer.step()   \n",
    "    \n",
    "    model.eval()\n",
    "    train_accuracy = accuracy_check(loader=train_loader, model=model, device=device)\n",
    "    test_accuracy = accuracy_check(loader=test_loader, model=model, device=device)\n",
    "\n",
    "    print('Epoch: {}. Tr Acc: {}. Te Acc: {}.'.format(epoch+1, train_accuracy, test_accuracy))\n",
    " \n",
    "    if epoch >= (args.ep-10):\n",
    "        test_acc_list.extend([test_accuracy])\n",
    "        train_acc_list.extend([train_accuracy])\n",
    "            \n",
    "avg_test_acc = np.mean(test_acc_list)\n",
    "\n",
    "avg_train_acc = np.mean(train_acc_list)\n",
    "\n",
    "print(\"Average Test Accuracy over Last 10 Epochs:\", avg_test_acc)\n",
    "\n",
    "print(\"Average Training Accuracy over Last 10 Epochs:\", avg_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8216a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b22c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
