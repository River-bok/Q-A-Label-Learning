{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wDf-gKwgHQc6"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "l8enVY2x-WV_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YvD5g-iyHXv0"
   },
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DfLerBb9-WWD"
   },
   "outputs": [],
   "source": [
    "class QuestionType(enum.Enum):\n",
    "    WHICH_ONE = 'which_one'\n",
    "    IS_IN = 'is_in'\n",
    "class DataSet(enum.Enum):\n",
    "    MNIST = 'mnist'\n",
    "    FASHION = 'fashion'\n",
    "    KMNIST = 'kmnist'\n",
    "    CIFAR10 = 'cifar-10'\n",
    "\n",
    "get_question_type = {QuestionType.WHICH_ONE: 'which_one',\n",
    "                       QuestionType.IS_IN: 'is_in'}\n",
    "get_dataset_name = {DataSet.MNIST: 'mnist',\n",
    "                    DataSet.FASHION: 'fashion', \n",
    "                    DataSet.KMNIST: 'kmnist',\n",
    "                    DataSet.CIFAR10: 'cifar-10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def prepare_mnist_data(batch_size):\n",
    "    ordinary_train_dataset = datasets.MNIST(root='./data/mnist', train=True, transform=T.ToTensor(), download=True)\n",
    "    test_dataset = datasets.MNIST(root='./data/mnist', train=False, transform=T.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "def prepare_kmnist_data(batch_size):\n",
    "    ordinary_train_dataset = datasets.KMNIST(root='./data/KMNIST', train=True, transform=T.ToTensor(), download=True)\n",
    "    test_dataset = datasets.KMNIST(root='./data/KMNIST', train=False, transform=T.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "def prepare_fashion_data(batch_size):\n",
    "    ordinary_train_dataset = datasets.FashionMNIST(root='./data/FashionMnist', train=True, transform=T.ToTensor(), download=True)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data/FashionMnist', train=False, transform=T.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "#torchvision.datasets.CIFAR10.url=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "def prepare_cifar10_data(batch_size: int) -> [DataLoader, DataLoader, DataLoader, DataLoader]:\n",
    "    train_transform = T.Compose(\n",
    "        [T.ToTensor(), # transforms.RandomHorizontalFlip(), transforms.RandomCrop(32,4),\n",
    "         T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    test_transform = T.Compose(\n",
    "        [T.ToTensor(),\n",
    "         T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transform, download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=len(train_dataset.data), shuffle=False, num_workers=0)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "full_train_loader_fashion, train_loader_fashion, test_loader_fashion = prepare_fashion_data(256)\n",
    "datas_fashion, labels_fashion = next(iter(full_train_loader_fashion))\n",
    "full_train_loader_kmnist, train_loader_kmnist, test_loader_kmnist = prepare_kmnist_data(256)\n",
    "datas_kmnist, labels_kmnist = next(iter(full_train_loader_kmnist))\n",
    "full_train_loader_mnist, train_loader_mnist, test_loader_mnist = prepare_mnist_data(256)\n",
    "datas_mnist, labels_mnist = next(iter(full_train_loader_mnist))\n",
    "full_train_loader_cifar, train_loader_cifar, test_loader_cifar = prepare_cifar10_data(256)\n",
    "datas_cifar, labels_cifar = next(iter(full_train_loader_cifar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_dataloader(full_train_loader: DataLoader, batch_size: int) -> [DataLoader, DataLoader]:\n",
    "    datas, labels = next(iter(full_train_loader))\n",
    "    count_list, delete_index_list = [0 for i in range(10)], []\n",
    "    index_list = list(range(len(datas)))\n",
    "    random.shuffle(index_list)\n",
    "    for index in index_list:\n",
    "        target = labels[index].tolist()\n",
    "        if count_list[target] >= 1000:\n",
    "            delete_index_list.append(index)\n",
    "        else:\n",
    "            count_list[target] += 1\n",
    "    datas = np.delete(datas, delete_index_list, 0)\n",
    "    labels = np.delete(labels, delete_index_list, 0)\n",
    "    \n",
    "    random_train_matrix_dataset = torch.utils.data.TensorDataset(datas, labels.float())\n",
    "    full_random_train_loader = torch.utils.data.DataLoader(dataset=random_train_matrix_dataset, batch_size=labels.shape[0], shuffle=False, num_workers=0)\n",
    "    random_train_loader = torch.utils.data.DataLoader(dataset=random_train_matrix_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return full_random_train_loader, random_train_loader\n",
    "full_random_train_loader_cifar, random_train_loader_mnist = get_random_dataloader(full_train_loader_mnist, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUKSD9Z8-WWE",
    "outputId": "20a09a6d-c819-4a04-ad66-7d90f0cd226b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_Q_A_label(single_class_assigned: int, \n",
    "                         question_type: int, \n",
    "                         all_class_size: int, \n",
    "                         question_class_size: int) -> list:\n",
    "    \n",
    "    all_class_set = range(all_class_size)\n",
    "    question_class_set = random.sample(all_class_set, question_class_size)\n",
    "    temporary_label_list = [0 for i in range(all_class_size)]\n",
    "    \n",
    "    if question_type == QuestionType.WHICH_ONE:\n",
    "        if single_class_assigned in question_class_set:\n",
    "            for i in [single_class_assigned]:\n",
    "                temporary_label_list[i] = 1\n",
    "        \n",
    "        else:\n",
    "            comp_question_class_set = [i for i in all_class_set if i not in question_class_set]\n",
    "            for i in comp_question_class_set:\n",
    "                temporary_label_list[i] = 1\n",
    "        \n",
    "    elif question_type == QuestionType.IS_IN:\n",
    "        if single_class_assigned in question_class_set:\n",
    "            for i in question_class_set:\n",
    "                temporary_label_list[i] = 1\n",
    "        \n",
    "        else:\n",
    "            comp_question_class_set = [i for i in all_class_set if i not in question_class_set]\n",
    "            for i in comp_question_class_set:\n",
    "                temporary_label_list[i] = 1\n",
    "    \n",
    "    return temporary_label_list\n",
    "\n",
    "temporary_label_list = generate_Q_A_label(single_class_assigned=0, \n",
    "                                            question_type=QuestionType.WHICH_ONE, \n",
    "                                            all_class_size=10, \n",
    "                                            question_class_size=9)\n",
    "\n",
    "temporary_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4ROvIYgF-WWF"
   },
   "outputs": [],
   "source": [
    "def generate_Q_A_labels(labels_tensor: torch.tensor, \n",
    "                          question_type: QuestionType,\n",
    "                          all_class_size: int,\n",
    "                          question_class_size: int) -> torch.tensor:\n",
    "    \n",
    "    labels_list = [int(i) for i in labels_tensor.tolist()]\n",
    "    Q_A_labels_list = []\n",
    "    \n",
    "    for label in labels_list:\n",
    "        Q_A_label = generate_Q_A_label(single_class_assigned=label, \n",
    "                                          question_type=question_type,  # change out of this function\n",
    "                                          all_class_size=all_class_size, # change out of this function\n",
    "                                          question_class_size=question_class_size) # change out of this function\n",
    "        Q_A_labels_list.append(Q_A_label)\n",
    "        \n",
    "    Q_A_labels_tensor = torch.tensor(Q_A_labels_list)\n",
    "    \n",
    "    return Q_A_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6FaB4-l-WWF",
    "outputId": "6c57d845-ed30-42a2-c8eb-88da971ed459"
   },
   "outputs": [],
   "source": [
    "def generate_dataloader_with_Q_A_label(full_random_train_loader: DataLoader, \n",
    "                                       question_type: QuestionType,\n",
    "                                       question_size: int,\n",
    "                                       batch_size: int) ->[DataLoader, int, int]:\n",
    "    \n",
    "    for i, (datas, labels) in enumerate(full_random_train_loader):\n",
    "        all_class_size = torch.max(labels) + 1 # K is number of classes, full_train_loader is full batch\n",
    "        all_class_size = int(all_class_size.tolist())\n",
    "        \n",
    "    Q_A_labels_tensor = generate_Q_A_labels(labels, question_type, all_class_size, question_size)\n",
    "    Q_A_labels_matrix_dataset = torch.utils.data.TensorDataset(datas, Q_A_labels_tensor.float())\n",
    "    \n",
    "    Q_A_labels_matrix_train_loader = torch.utils.data.DataLoader(dataset=Q_A_labels_matrix_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    full_Q_A_labels_matrix_train_loader = torch.utils.data.DataLoader(dataset=Q_A_labels_matrix_dataset, batch_size=datas.shape[0], shuffle=False, num_workers=0)\n",
    "    dimension = int(datas.reshape(-1).shape[0]/datas.shape[0])\n",
    "    return full_Q_A_labels_matrix_train_loader, Q_A_labels_matrix_train_loader, dimension, all_class_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQyJl_0N-WWG",
    "outputId": "df832967-0186-4a49-86a6-ae282f2195fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from traitlets.traitlets import Integer\n",
    "# Unclear!!!\n",
    "DataSet_2_DataLoader_generator = {\n",
    "    DataSet.MNIST: prepare_mnist_data,\n",
    "    DataSet.FASHION: prepare_fashion_data,\n",
    "    DataSet.KMNIST: prepare_kmnist_data,\n",
    "    DataSet.CIFAR10: prepare_cifar10_data\n",
    "}\n",
    "\n",
    "def preparence(dataset: DataSet, batch_size: int):\n",
    "    dataloader_generator = DataSet_2_DataLoader_generator[dataset]\n",
    "    full_train_loader, train_loader, test_loader = dataloader_generator(batch_size)\n",
    "    full_random_train_loader, random_train_loader = get_random_dataloader(full_train_loader, batch_size)\n",
    "    \n",
    "    return full_random_train_loader, random_train_loader, test_loader\n",
    "\n",
    "full_random_train_loader, random_train_loader, test_loader = preparence(DataSet.CIFAR10, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_scalar_to_array(labels: torch.tensor, all_class_size: int) -> torch.Tensor:\n",
    "    outputs = []\n",
    "    for label in labels:\n",
    "        label = int(label.tolist())\n",
    "        label_full_size = [0] * all_class_size\n",
    "        label_full_size[label] = 1\n",
    "        outputs.append(label_full_size)\n",
    "    \n",
    "    outputs = torch.tensor(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_uttS2WgHgpB"
   },
   "source": [
    "# Model excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "izJeh7tu-WWG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mae_loss(outputs, labels):\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    loss_fn = nn.L1Loss(reduction='none')\n",
    "    loss_matrix = loss_fn(sm_outputs, labels.float())\n",
    "    sample_loss = loss_matrix.sum(dim=-1)\n",
    "    return sample_loss\n",
    "    \n",
    "def mse_loss(outputs, labels):\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    loss_matrix = loss_fn(sm_outputs, labels.float())\n",
    "    sample_loss = loss_matrix.sum(dim=-1)\n",
    "    return sample_loss\n",
    "\n",
    "def gce_loss(outputs, labels):\n",
    "    q = 0.7\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    pow_outputs = torch.pow(sm_outputs, q)\n",
    "    sample_loss = (1-(pow_outputs*labels).sum(dim=1))/q # n\n",
    "    return sample_loss\n",
    "\n",
    "def phuber_ce_loss(outputs, labels):\n",
    "    trunc_point = 0.1\n",
    "    n = labels.shape[0]\n",
    "    soft_max = nn.Softmax(dim=1)\n",
    "    sm_outputs = soft_max(outputs)\n",
    "    final_outputs = sm_outputs * labels\n",
    "    final_confidence = final_outputs.sum(dim=1)\n",
    "    ce_index = (final_confidence > trunc_point)\n",
    "    sample_loss = torch.zeros(n).to(device)\n",
    "\n",
    "    if ce_index.sum() > 0:\n",
    "        ce_outputs = outputs[ce_index,:]\n",
    "        logsm = nn.LogSoftmax(dim=-1)\n",
    "        logsm_outputs = logsm(ce_outputs)\n",
    "        final_ce_outputs = logsm_outputs * labels[ce_index,:]\n",
    "        sample_loss[ce_index] = - final_ce_outputs.sum(dim=-1)\n",
    "\n",
    "    linear_index = (final_confidence <= trunc_point)\n",
    "\n",
    "    if linear_index.sum() > 0:\n",
    "        sample_loss[linear_index] = -math.log(trunc_point) + (-1/trunc_point)*final_confidence[linear_index] + 1\n",
    "\n",
    "    return sample_loss\n",
    "\n",
    "def ce_loss(outputs, labels):\n",
    "    logsm = nn.LogSoftmax(dim=1)\n",
    "    logsm_outputs = logsm(outputs)\n",
    "    final_outputs = logsm_outputs * labels\n",
    "    sample_loss = - final_outputs.sum(dim=1)\n",
    "    return sample_loss\n",
    "\n",
    "def W_O_loss(loss_fn_, outputs, labels, device, question_class_size, all_class_size):\n",
    "    n, k = labels.shape[0], labels.shape[1]\n",
    "    temp_loss = torch.zeros(n, k).to(device)\n",
    "    for i in range(k):\n",
    "        tempY = torch.zeros(n, k).to(device)\n",
    "        tempY[:, i] = 1.0\n",
    "        temp_loss[:, i] = loss_fn_(outputs, tempY)\n",
    "        \n",
    "    candidate_loss = (temp_loss * labels).sum(dim=1)\n",
    "    noncandidate_loss = (temp_loss * (1-labels)).sum(dim=1)\n",
    "    total_loss = candidate_loss - ((all_class_size - question_class_size) * (all_class_size - question_class_size - 1))/(question_class_size * (2*all_class_size - question_class_size - 1.0)) * noncandidate_loss\n",
    "    average_loss = total_loss.mean()\n",
    "    return average_loss\n",
    "\n",
    "def I_I_loss(loss_fn_, outputs, labels, device, question_class_size, all_class_size):\n",
    "    n, k = labels.shape[0], labels.shape[1]\n",
    "    temp_loss = torch.zeros(n, k).to(device)\n",
    "    for i in range(k):\n",
    "        tempY = torch.zeros(n, k).to(device)\n",
    "        tempY[:, i] = 1.0\n",
    "        temp_loss[:, i] = loss_fn_(outputs, tempY)\n",
    "        \n",
    "    candidate_loss = (temp_loss * labels).sum(dim=1)\n",
    "    noncandidate_loss = (temp_loss * (1-labels)).sum(dim=1)\n",
    "    total_loss = candidate_loss - (2*question_class_size**2 + all_class_size**2 - all_class_size*(2*question_class_size + 1))/(2*question_class_size * (all_class_size - question_class_size)) * noncandidate_loss\n",
    "    average_loss = total_loss.mean()\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "7eQ0uYqy-WWH"
   },
   "outputs": [],
   "source": [
    "class mlp_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(mlp_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, self.num_flat_features(x))\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "HiI0NlQ1-WWH"
   },
   "outputs": [],
   "source": [
    "def accuracy_check(loader, model, device):\n",
    "    with torch.no_grad():\n",
    "        total, num_samples = 0, 0\n",
    "        for images, labels in loader:\n",
    "            labels, images = labels.to(device), images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += (predicted == labels).sum().item()\n",
    "            num_samples += labels.size(0) \n",
    "    return total / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_check(loader, model, criterion, device, all_class_size):\n",
    "    with torch.no_grad():\n",
    "        total, num_samples = 0, 0\n",
    "        for images, labels in loader:\n",
    "            labels, images = labels.to(device), images.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = transform_scalar_to_array(labels, all_class_size)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(outputs,labels)\n",
    "            total += loss.sum().item()\n",
    "            num_samples += labels.size(0) \n",
    "    return total / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "naI292b5-WWH"
   },
   "outputs": [],
   "source": [
    "def show_loss(epoch,max_epoch, loss):\n",
    "    print('TRAIN EPOCH[{:03}/{:03}] LOSS:{:03f}%'.format(epoch, max_epoch, loss))    \n",
    "def show_eval_loss(epoch,max_epoch, loss, is_val):\n",
    "    if is_val:\n",
    "        print('EVAL TEST EPOCH[{:03}/{:03}] LOSS:{:03f}%'.format(epoch, max_epoch, loss))\n",
    "    else:\n",
    "        print('EVAL TRAIN EPOCH[{:03}/{:03}] LOSS:{:03f}%'.format(epoch, max_epoch, loss))\n",
    "def show_acc(epoch,max_epoch, acc, is_val):\n",
    "    if is_val:\n",
    "        print('TEST EPOCH[{:03}/{:03}] ACC:{:03f}%'.format(epoch, max_epoch, acc*100))\n",
    "    else:\n",
    "        print('TRAIN EPOCH[{:03}/{:03}] ACC:{:03f}%'.format(epoch, max_epoch, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "Qr7jT0aV-WWI"
   },
   "outputs": [],
   "source": [
    "def Train(model, \n",
    "          question_type, \n",
    "          question_class_size, \n",
    "          all_class_size, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          data_loader, \n",
    "          device, \n",
    "          epoch, \n",
    "          max_epoch):\n",
    "        \n",
    "    total_loss_train = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for n, (data, label) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(data)\n",
    "        if question_type == QuestionType.WHICH_ONE:\n",
    "            loss = W_O_loss(criterion, output, label.float(), device, question_class_size, all_class_size)\n",
    "        elif question_type == QuestionType.IS_IN:\n",
    "            loss = I_I_loss(criterion, output, label.float(), device, question_class_size, all_class_size)\n",
    "        else:\n",
    "            loss = criterion(output,label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "    show_loss(epoch+1, max_epoch, total_loss_train/(n+1))\n",
    "    print()\n",
    "\n",
    "    return total_loss_train/(n+1), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval(index,\n",
    "        model, \n",
    "        evaluation_data_loader_train, \n",
    "        evaluation_data_loader_test,\n",
    "        all_class_size,\n",
    "        criterion,\n",
    "        device, \n",
    "        epoch, \n",
    "        max_epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    if index == 'acc':\n",
    "        total_acc_train = accuracy_check(evaluation_data_loader_train, model, device)\n",
    "        total_acc_test = accuracy_check(evaluation_data_loader_test, model, device)\n",
    "\n",
    "        show_acc(epoch+1, max_epoch, total_acc_train, is_val=False)\n",
    "        print()\n",
    "        show_acc(epoch+1, max_epoch, total_acc_test, is_val=True)\n",
    "        print()\n",
    "\n",
    "        return total_acc_train , total_acc_test, model\n",
    "    elif index == 'loss':\n",
    "        total_loss_train = loss_check(evaluation_data_loader_train, model, criterion, device, all_class_size)\n",
    "        total_loss_test = loss_check(evaluation_data_loader_test, model, criterion, device, all_class_size)\n",
    "        show_eval_loss(epoch+1, max_epoch, total_loss_train, is_val=False)\n",
    "        print()\n",
    "        show_eval_loss(epoch+1, max_epoch, total_loss_test, is_val=True)\n",
    "        print()\n",
    "\n",
    "        return total_loss_train , total_loss_test, model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model = {mlp_model: 'MLP'}\n",
    "get_loss_function = {ce_loss: 'CE_LOSS', \n",
    "                     mae_loss: 'MAE_LOSS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "U3UMkIUz-WWI"
   },
   "outputs": [],
   "source": [
    "def Q_A_label_Train_Eval(index,\n",
    "                          dataset, \n",
    "                          question_type, \n",
    "                          question_class_size, \n",
    "                          model_name, \n",
    "                          batch_size, \n",
    "                          loss_fn, \n",
    "                          EPOCHS):\n",
    "    print(f'START: {question_type}, {question_class_size}')\n",
    "    \n",
    "    full_Q_A_labels_matrix_train_loader, Q_A_labels_matrix_train_loader, dimension, all_class_size = generate_dataloader_with_Q_A_label(full_random_train_loader,\n",
    "                                                                                                                                       question_type,\n",
    "                                                                                                                                       question_class_size,\n",
    "                                                                                                                                       batch_size)\n",
    "    full_Q_A_datas, full_Q_A_labels = next(iter(full_Q_A_labels_matrix_train_loader))\n",
    "    counter = collections.Counter(torch.sum(full_Q_A_labels, 1).int().tolist())\n",
    "    print(counter)\n",
    "    \n",
    "    \n",
    "    #DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    DEVICE = torch.device('mps')\n",
    "    index=index\n",
    "    #model = model_name(input_dim=dimension, hidden_dim=500, output_dim=all_class_size).to(DEVICE)\n",
    "    model = model_name\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    train_index_list = []\n",
    "    eval_train_index_list = []\n",
    "    eval_test_index_list = []\n",
    "    #results_df = pd.DataFrame(columns=[\"index\", \"dataset\", \"epoch\", \"question_type\", \"question_class_size\", \"model\", \"loss_function\", \"train_index\", \"train_index\", \"test_index\"])\n",
    "    results_df = []\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        train_index, model = Train(model=model, \n",
    "                                  question_type=question_type,\n",
    "                                  question_class_size=question_class_size,\n",
    "                                  all_class_size=all_class_size,\n",
    "                                  criterion=loss_fn,\n",
    "                                  optimizer=optimizer, \n",
    "                                  data_loader=Q_A_labels_matrix_train_loader,\n",
    "                                  device=DEVICE, \n",
    "                                  epoch=epoch, \n",
    "                                  max_epoch=EPOCHS)\n",
    "        \n",
    "        eval_train_index, eval_test_index, model = Eval(index=index,\n",
    "                                                        model=model, \n",
    "                                                        evaluation_data_loader_train=random_train_loader, \n",
    "                                                        evaluation_data_loader_test=test_loader,\n",
    "                                                        all_class_size=all_class_size,\n",
    "                                                        criterion=loss_fn,\n",
    "                                                        device=DEVICE, \n",
    "                                                        epoch=epoch, \n",
    "                                                        max_epoch=EPOCHS)\n",
    "\n",
    "        train_index_list.append(train_index)\n",
    "        eval_train_index_list.append(eval_train_index)\n",
    "        eval_test_index_list.append(eval_test_index)\n",
    "\n",
    "        result_df = pd.DataFrame({'index': [index],\n",
    "                                      'dataset': [get_dataset_name[dataset]],\n",
    "                                      'epoch': [epoch+1],\n",
    "                                      'question_type': [get_question_type[question_type]], \n",
    "                                      'question_class_size': [question_class_size], \n",
    "                                      'model': [get_model[model_name]],\n",
    "                                      'loss_function': [get_loss_function[loss_fn]],\n",
    "                                      'train_index': [train_index],\n",
    "                                      'eval_train_index': [eval_train_index],\n",
    "                                      'eval_test_index': [eval_test_index]})\n",
    "        results_df.append(result_df)\n",
    "    \n",
    "    results_df = pd.concat(results_df, axis=0)\n",
    "    \n",
    "    if question_type == QuestionType.WHICH_ONE:\n",
    "        if question_class_size != 9: \n",
    "            results_df['label_size_cand'] = counter[1]\n",
    "            results_df['label_size_comp'] = counter[all_class_size - question_class_size]\n",
    "        else:\n",
    "            results_df['label_size_cand'] = counter[1]\n",
    "            results_df['label_size_comp'] = np.nan\n",
    "    elif question_type == QuestionType.IS_IN:\n",
    "        if question_class_size != 5: \n",
    "            results_df['label_size_cand'] = counter[question_class_size]\n",
    "            results_df['label_size_comp'] = counter[all_class_size - question_class_size]\n",
    "        else:\n",
    "            results_df['label_size_cand'] = counter[question_class_size]\n",
    "            results_df['label_size_comp'] = np.nan\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "full_random_train_loader, random_train_loader, test_loader = preparence(DataSet.CIFAR10, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: QuestionType.WHICH_ONE, 3\n",
      "Counter({7: 7009, 1: 2991})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a029b293c7814989895d53eb9edb83c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[39m=\u001b[39m Q_A_label_Train_Eval(index\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m                                 dataset\u001b[39m=\u001b[39;49mDataSet\u001b[39m.\u001b[39;49mCIFAR10, \n\u001b[1;32m      3\u001b[0m                                 question_type\u001b[39m=\u001b[39;49mQuestionType\u001b[39m.\u001b[39;49mWHICH_ONE, \n\u001b[1;32m      4\u001b[0m                                 question_class_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m                                 model_name\u001b[39m=\u001b[39;49mmodel_name, \n\u001b[1;32m      6\u001b[0m                                 batch_size\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m                                 loss_fn\u001b[39m=\u001b[39;49mmae_loss, \n\u001b[1;32m      8\u001b[0m                                 EPOCHS\u001b[39m=\u001b[39;49m\u001b[39m800\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[159], line 33\u001b[0m, in \u001b[0;36mQ_A_label_Train_Eval\u001b[0;34m(index, dataset, question_type, question_class_size, model_name, batch_size, loss_fn, EPOCHS)\u001b[0m\n\u001b[1;32m     31\u001b[0m results_df \u001b[39m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(EPOCHS)):\n\u001b[0;32m---> 33\u001b[0m     train_index, model \u001b[39m=\u001b[39m Train(model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m     34\u001b[0m                               question_type\u001b[39m=\u001b[39;49mquestion_type,\n\u001b[1;32m     35\u001b[0m                               question_class_size\u001b[39m=\u001b[39;49mquestion_class_size,\n\u001b[1;32m     36\u001b[0m                               all_class_size\u001b[39m=\u001b[39;49mall_class_size,\n\u001b[1;32m     37\u001b[0m                               criterion\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     38\u001b[0m                               optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m     39\u001b[0m                               data_loader\u001b[39m=\u001b[39;49mQ_A_labels_matrix_train_loader,\n\u001b[1;32m     40\u001b[0m                               device\u001b[39m=\u001b[39;49mDEVICE, \n\u001b[1;32m     41\u001b[0m                               epoch\u001b[39m=\u001b[39;49mepoch, \n\u001b[1;32m     42\u001b[0m                               max_epoch\u001b[39m=\u001b[39;49mEPOCHS)\n\u001b[1;32m     44\u001b[0m     eval_train_index, eval_test_index, model \u001b[39m=\u001b[39m Eval(index\u001b[39m=\u001b[39mindex,\n\u001b[1;32m     45\u001b[0m                                                     model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m     46\u001b[0m                                                     evaluation_data_loader_train\u001b[39m=\u001b[39mrandom_train_loader, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m                                                     epoch\u001b[39m=\u001b[39mepoch, \n\u001b[1;32m     52\u001b[0m                                                     max_epoch\u001b[39m=\u001b[39mEPOCHS)\n\u001b[1;32m     54\u001b[0m     train_index_list\u001b[39m.\u001b[39mappend(train_index)\n",
      "Cell \u001b[0;32mIn[155], line 19\u001b[0m, in \u001b[0;36mTrain\u001b[0;34m(model, question_type, question_class_size, all_class_size, criterion, optimizer, data_loader, device, epoch, max_epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m question_type \u001b[39m==\u001b[39m QuestionType\u001b[39m.\u001b[39mWHICH_ONE:\n\u001b[1;32m     21\u001b[0m     loss \u001b[39m=\u001b[39m W_O_loss(criterion, output, label\u001b[39m.\u001b[39mfloat(), device, question_class_size, all_class_size)\n",
      "File \u001b[0;32m~/Python/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[139], line 83\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 83\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     84\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(out)\n\u001b[1;32m     85\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n",
      "File \u001b[0;32m~/Python/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Python/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Python/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "result_df = Q_A_label_Train_Eval(index='loss',\n",
    "                                dataset=DataSet.CIFAR10, \n",
    "                                question_type=QuestionType.WHICH_ONE, \n",
    "                                question_class_size=3, \n",
    "                                model_name=model_name, \n",
    "                                batch_size=500, \n",
    "                                loss_fn=mae_loss, \n",
    "                                EPOCHS=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "i5I4hQib-WWI",
    "outputId": "6dc1435a-248a-4f8e-92bc-121f7695d939",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bc743273d84e68b5a9539c07edb81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f8ca25a76049ce8e4c9d25d6c4a3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b1550af9294214a9163f50005e6ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c913360ed14cba9e79159e2d6b1392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: QuestionType.WHICH_ONE, 1\n",
      "Counter({9: 8984, 1: 1016})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc24fe13020d41b997c856b5a17b55c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN EPOCH[001/800] LOSS:1.438538%\n",
      "\n",
      "EVAL TRAIN EPOCH[001/800] LOSS:1.139098%\n",
      "\n",
      "EVAL TEST EPOCH[001/800] LOSS:1.119246%\n",
      "\n",
      "TRAIN EPOCH[002/800] LOSS:0.667375%\n",
      "\n",
      "EVAL TRAIN EPOCH[002/800] LOSS:0.575509%\n",
      "\n",
      "EVAL TEST EPOCH[002/800] LOSS:0.541956%\n",
      "\n",
      "TRAIN EPOCH[003/800] LOSS:0.324756%\n",
      "\n",
      "EVAL TRAIN EPOCH[003/800] LOSS:0.410514%\n",
      "\n",
      "EVAL TEST EPOCH[003/800] LOSS:0.382617%\n",
      "\n",
      "TRAIN EPOCH[004/800] LOSS:0.200409%\n",
      "\n",
      "EVAL TRAIN EPOCH[004/800] LOSS:0.347897%\n",
      "\n",
      "EVAL TEST EPOCH[004/800] LOSS:0.326304%\n",
      "\n",
      "TRAIN EPOCH[005/800] LOSS:0.138972%\n",
      "\n",
      "EVAL TRAIN EPOCH[005/800] LOSS:0.311073%\n",
      "\n",
      "EVAL TEST EPOCH[005/800] LOSS:0.293919%\n",
      "\n",
      "TRAIN EPOCH[006/800] LOSS:0.097857%\n",
      "\n",
      "EVAL TRAIN EPOCH[006/800] LOSS:0.288278%\n",
      "\n",
      "EVAL TEST EPOCH[006/800] LOSS:0.270293%\n",
      "\n",
      "TRAIN EPOCH[007/800] LOSS:0.067674%\n",
      "\n",
      "EVAL TRAIN EPOCH[007/800] LOSS:0.277312%\n",
      "\n",
      "EVAL TEST EPOCH[007/800] LOSS:0.262006%\n",
      "\n",
      "TRAIN EPOCH[008/800] LOSS:0.042067%\n",
      "\n",
      "EVAL TRAIN EPOCH[008/800] LOSS:0.259741%\n",
      "\n",
      "EVAL TEST EPOCH[008/800] LOSS:0.249509%\n",
      "\n",
      "TRAIN EPOCH[009/800] LOSS:0.022887%\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m question_type \u001b[39min\u001b[39;00m tqdm([QuestionType\u001b[39m.\u001b[39mWHICH_ONE, QuestionType\u001b[39m.\u001b[39mIS_IN]):\n\u001b[1;32m      8\u001b[0m         \u001b[39mfor\u001b[39;00m question_size \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)):\n\u001b[0;32m----> 9\u001b[0m                 result_df \u001b[39m=\u001b[39m Q_A_label_Train_Eval(index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m     10\u001b[0m                                                  dataset\u001b[39m=\u001b[39;49mdataset_name, \n\u001b[1;32m     11\u001b[0m                                                  question_type\u001b[39m=\u001b[39;49mquestion_type, \n\u001b[1;32m     12\u001b[0m                                                  question_class_size\u001b[39m=\u001b[39;49mquestion_size, \n\u001b[1;32m     13\u001b[0m                                                  model_name\u001b[39m=\u001b[39;49mmodel_name, \n\u001b[1;32m     14\u001b[0m                                                  batch_size\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, \n\u001b[1;32m     15\u001b[0m                                                  loss_fn\u001b[39m=\u001b[39;49mmae_loss, \n\u001b[1;32m     16\u001b[0m                                                  EPOCHS\u001b[39m=\u001b[39;49m\u001b[39m800\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m                 results\u001b[39m.\u001b[39mappend(result_df)\n\u001b[1;32m     18\u001b[0m output \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(results, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[124], line 43\u001b[0m, in \u001b[0;36mQ_A_label_Train_Eval\u001b[0;34m(index, dataset, question_type, question_class_size, model_name, batch_size, loss_fn, EPOCHS)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(EPOCHS)):\n\u001b[1;32m     32\u001b[0m     train_index, model \u001b[39m=\u001b[39m Train(model\u001b[39m=\u001b[39mmodel, \n\u001b[1;32m     33\u001b[0m                               question_type\u001b[39m=\u001b[39mquestion_type,\n\u001b[1;32m     34\u001b[0m                               question_class_size\u001b[39m=\u001b[39mquestion_class_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m                               epoch\u001b[39m=\u001b[39mepoch, \n\u001b[1;32m     41\u001b[0m                               max_epoch\u001b[39m=\u001b[39mEPOCHS)\n\u001b[0;32m---> 43\u001b[0m     eval_train_index, eval_test_index, model \u001b[39m=\u001b[39m Eval(index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m     44\u001b[0m                                                     model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m     45\u001b[0m                                                     evaluation_data_loader_train\u001b[39m=\u001b[39;49mrandom_train_loader, \n\u001b[1;32m     46\u001b[0m                                                     evaluation_data_loader_test\u001b[39m=\u001b[39;49mtest_loader,\n\u001b[1;32m     47\u001b[0m                                                     all_class_size\u001b[39m=\u001b[39;49mall_class_size,\n\u001b[1;32m     48\u001b[0m                                                     criterion\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     49\u001b[0m                                                     device\u001b[39m=\u001b[39;49mDEVICE, \n\u001b[1;32m     50\u001b[0m                                                     epoch\u001b[39m=\u001b[39;49mepoch, \n\u001b[1;32m     51\u001b[0m                                                     max_epoch\u001b[39m=\u001b[39;49mEPOCHS)\n\u001b[1;32m     53\u001b[0m     train_index_list\u001b[39m.\u001b[39mappend(train_index)\n\u001b[1;32m     54\u001b[0m     eval_train_index_list\u001b[39m.\u001b[39mappend(eval_train_index)\n",
      "Cell \u001b[0;32mIn[70], line 23\u001b[0m, in \u001b[0;36mEval\u001b[0;34m(index, model, evaluation_data_loader_train, evaluation_data_loader_test, all_class_size, criterion, device, epoch, max_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m total_acc_train , total_acc_test, model\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m index \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     total_loss_train \u001b[39m=\u001b[39m loss_check(evaluation_data_loader_train, model, criterion, device, all_class_size)\n\u001b[1;32m     24\u001b[0m     total_loss_test \u001b[39m=\u001b[39m loss_check(evaluation_data_loader_test, model, criterion, device, all_class_size)\n\u001b[1;32m     25\u001b[0m     show_eval_loss(epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, max_epoch, total_loss_train, is_val\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[67], line 7\u001b[0m, in \u001b[0;36mloss_check\u001b[0;34m(loader, model, criterion, device, all_class_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m labels, images \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device), images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m----> 7\u001b[0m labels \u001b[39m=\u001b[39m transform_scalar_to_array(labels, all_class_size)\n\u001b[1;32m      8\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,labels)\n",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m, in \u001b[0;36mtransform_scalar_to_array\u001b[0;34m(labels, all_class_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels:\n\u001b[0;32m----> 4\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(label\u001b[39m.\u001b[39;49mtolist())\n\u001b[1;32m      5\u001b[0m     label_full_size \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m all_class_size\n\u001b[1;32m      6\u001b[0m     label_full_size[label] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index = 'loss'\n",
    "for dataset_name in tqdm([DataSet.MNIST, DataSet.KMNIST, DataSet.FASHION]):\n",
    "    results = []\n",
    "    full_random_train_loader, random_train_loader, test_loader = preparence(dataset_name, 512)\n",
    "    model_name = mlp_model\n",
    "    for itr in tqdm(range(1,6)):\n",
    "        for question_type in tqdm([QuestionType.WHICH_ONE, QuestionType.IS_IN]):\n",
    "            for question_size in tqdm(range(1, 10)):\n",
    "                    result_df = Q_A_label_Train_Eval(index=index,\n",
    "                                                     dataset=dataset_name, \n",
    "                                                     question_type=question_type, \n",
    "                                                     question_class_size=question_size, \n",
    "                                                     model_name=model_name, \n",
    "                                                     batch_size=500, \n",
    "                                                     loss_fn=mae_loss, \n",
    "                                                     EPOCHS=800)\n",
    "                    results.append(result_df)\n",
    "    output = pd.concat(results, axis=0)\n",
    "    output.to_csv(f'all_result_{get_dataset_name[dataset_name]}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
