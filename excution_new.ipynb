{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDf-gKwgHQc6"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l8enVY2x-WV_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.dataset import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvD5g-iyHXv0"
   },
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DfLerBb9-WWD"
   },
   "outputs": [],
   "source": [
    "class QuestionType(enum.Enum):\n",
    "    WHICH_ONE = 'which_one'\n",
    "    IS_IN = 'is_in'\n",
    "class DataSet(enum.Enum):\n",
    "    MNIST = 'mnist'\n",
    "    FASHION = 'fashion'\n",
    "    KMNIST = 'kmnist'\n",
    "\n",
    "get_question_type = {QuestionType.WHICH_ONE: 'which_one',\n",
    "                       QuestionType.IS_IN: 'is_in'}\n",
    "get_dataset_name = {DataSet.MNIST: 'mnist',\n",
    "                    DataSet.FASHION: 'fashion', \n",
    "                    DataSet.KMNIST: 'kmnist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:28<00:00, 919616.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 96361.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:10<00:00, 415169.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 5436122.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to ./data/KMNIST/KMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18165135/18165135 [00:02<00:00, 8392351.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/KMNIST/KMNIST/raw/train-images-idx3-ubyte.gz to ./data/KMNIST/KMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to ./data/KMNIST/KMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29497/29497 [00:00<00:00, 1704828.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/KMNIST/KMNIST/raw/train-labels-idx1-ubyte.gz to ./data/KMNIST/KMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to ./data/KMNIST/KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3041136/3041136 [00:00<00:00, 7064836.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/KMNIST/KMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/KMNIST/KMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to ./data/KMNIST/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5120/5120 [00:00<00:00, 13652152.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/KMNIST/KMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/KMNIST/KMNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 7117566.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 101368781.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 15490540.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 15538767.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_mnist_data(batch_size):\n",
    "    ordinary_train_dataset = datasets.MNIST(root='./data/mnist', train=True, transform=T.ToTensor(), download=True)\n",
    "    test_dataset = datasets.MNIST(root='./data/mnist', train=False, transform=T.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "def prepare_kmnist_data(batch_size):\n",
    "    ordinary_train_dataset = datasets.KMNIST(root='./data/KMNIST', train=True, transform=T.ToTensor(), download=True)\n",
    "    test_dataset = datasets.KMNIST(root='./data/KMNIST', train=False, transform=T.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "def prepare_fashion_data(batch_size):\n",
    "    ordinary_train_dataset = datasets.FashionMNIST(root='./data/FashionMnist', train=True, transform=T.ToTensor(), download=True)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data/FashionMnist', train=False, transform=T.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    full_train_loader = torch.utils.data.DataLoader(dataset=ordinary_train_dataset, batch_size=len(ordinary_train_dataset.data), shuffle=True, num_workers=0)\n",
    "    num_classes = len(ordinary_train_dataset.classes)\n",
    "    return full_train_loader, train_loader, test_loader\n",
    "\n",
    "full_train_loader_fashion, train_loader_fashion, test_loader_fashion = prepare_fashion_data(256)\n",
    "datas_fashion, labels_fashion = next(iter(full_train_loader_fashion))\n",
    "full_train_loader_kmnist, train_loader_kmnist, test_loader_kmnist = prepare_kmnist_data(256)\n",
    "datas_kmnist, labels_kmnist = next(iter(full_train_loader_kmnist))\n",
    "full_train_loader_mnist, train_loader_mnist, test_loader_mnist = prepare_mnist_data(256)\n",
    "datas_mnist, labels_mnist = next(iter(full_train_loader_mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_dataloader(full_train_loader, batch_size):\n",
    "    datas, labels = next(iter(full_train_loader))\n",
    "    count_list, delete_index_list = [0 for i in range(10)], []\n",
    "    index_list = list(range(len(datas)))\n",
    "    random.shuffle(index_list)\n",
    "    for index in index_list:\n",
    "        target = labels[index].tolist()\n",
    "        if count_list[target] >= 1000:\n",
    "            delete_index_list.append(index)\n",
    "        else:\n",
    "            count_list[target] += 1\n",
    "    datas = np.delete(datas, delete_index_list, 0)\n",
    "    labels = np.delete(labels, delete_index_list, 0)\n",
    "    \n",
    "    random_train_matrix_dataset = torch.utils.data.TensorDataset(datas, labels.float())\n",
    "    full_random_train_loader = torch.utils.data.DataLoader(dataset=random_train_matrix_dataset, batch_size=labels.shape[0], shuffle=False, num_workers=0)\n",
    "    random_train_loader = torch.utils.data.DataLoader(dataset=random_train_matrix_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return full_random_train_loader, random_train_loader\n",
    "full_random_train_loader_mnist, random_train_loader_mnist = get_random_dataloader(full_train_loader_mnist, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUKSD9Z8-WWE",
    "outputId": "20a09a6d-c819-4a04-ad66-7d90f0cd226b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_Q_A_label(single_class_assigned: int, \n",
    "                         question_type: int, \n",
    "                         all_class_size: int, \n",
    "                         question_class_size: int) -> list:\n",
    "    \n",
    "    all_class_set = range(all_class_size)\n",
    "    question_class_set = random.sample(all_class_set, question_class_size)\n",
    "    temporary_label_list = [0 for i in range(all_class_size)]\n",
    "    \n",
    "    if question_type == QuestionType.WHICH_ONE:\n",
    "        if single_class_assigned in question_class_set:\n",
    "            for i in [single_class_assigned]:\n",
    "                temporary_label_list[i] = 1\n",
    "        \n",
    "        else:\n",
    "            comp_question_class_set = [i for i in all_class_set if i not in question_class_set]\n",
    "            for i in comp_question_class_set:\n",
    "                temporary_label_list[i] = 1\n",
    "        \n",
    "    elif question_type == QuestionType.IS_IN:\n",
    "        if single_class_assigned in question_class_set:\n",
    "            for i in question_class_set:\n",
    "                temporary_label_list[i] = 1\n",
    "        \n",
    "        else:\n",
    "            comp_question_class_set = [i for i in all_class_set if i not in question_class_set]\n",
    "            for i in comp_question_class_set:\n",
    "                temporary_label_list[i] = 1\n",
    "    \n",
    "    return temporary_label_list\n",
    "\n",
    "temporary_label_list = generate_Q_A_label(single_class_assigned=0, \n",
    "                                            question_type=QuestionType.WHICH_ONE, \n",
    "                                            all_class_size=10, \n",
    "                                            question_class_size=9)\n",
    "\n",
    "temporary_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4ROvIYgF-WWF"
   },
   "outputs": [],
   "source": [
    "def generate_Q_A_labels(labels_tensor: torch.tensor, \n",
    "                          question_type: QuestionType,\n",
    "                          all_class_size: int,\n",
    "                          question_class_size: int) -> torch.tensor:\n",
    "    \n",
    "    labels_list = [int(i) for i in labels_tensor.tolist()]\n",
    "    Q_A_labels_list = []\n",
    "    \n",
    "    for label in labels_list:\n",
    "        Q_A_label = generate_Q_A_label(single_class_assigned=label, \n",
    "                                          question_type=question_type,  # change out of this function\n",
    "                                          all_class_size=all_class_size, # change out of this function\n",
    "                                          question_class_size=question_class_size) # change out of this function\n",
    "        Q_A_labels_list.append(Q_A_label)\n",
    "        \n",
    "    Q_A_labels_tensor = torch.tensor(Q_A_labels_list)\n",
    "    \n",
    "    return Q_A_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6FaB4-l-WWF",
    "outputId": "6c57d845-ed30-42a2-c8eb-88da971ed459"
   },
   "outputs": [],
   "source": [
    "def generate_dataloader_with_Q_A_label(full_random_train_loader: DataLoader, \n",
    "                                       question_type: QuestionType,\n",
    "                                       question_size: int,\n",
    "                                       batch_size: int) ->[DataLoader, int, int]:\n",
    "    \n",
    "    for i, (datas, labels) in enumerate(full_random_train_loader):\n",
    "        all_class_size = torch.max(labels) + 1 # K is number of classes, full_train_loader is full batch\n",
    "        all_class_size = int(all_class_size.tolist())\n",
    "        \n",
    "    Q_A_labels_tensor = generate_Q_A_labels(labels, question_type, all_class_size, question_size)\n",
    "    Q_A_labels_matrix_dataset = torch.utils.data.TensorDataset(datas, Q_A_labels_tensor.float())\n",
    "    \n",
    "    Q_A_labels_matrix_train_loader = torch.utils.data.DataLoader(dataset=Q_A_labels_matrix_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    full_Q_A_labels_matrix_train_loader = torch.utils.data.DataLoader(dataset=Q_A_labels_matrix_dataset, batch_size=datas.shape[0], shuffle=False, num_workers=0)\n",
    "    dimension = int(datas.reshape(-1).shape[0]/datas.shape[0])\n",
    "    return full_Q_A_labels_matrix_train_loader, Q_A_labels_matrix_train_loader, dimension, all_class_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQyJl_0N-WWG",
    "outputId": "df832967-0186-4a49-86a6-ae282f2195fb"
   },
   "outputs": [],
   "source": [
    "from traitlets.traitlets import Integer\n",
    "# Unclear!!!\n",
    "DataSet_2_DataLoader_generator = {\n",
    "    DataSet.MNIST: prepare_mnist_data,\n",
    "    DataSet.FASHION: prepare_fashion_data,\n",
    "    DataSet.KMNIST: prepare_kmnist_data\n",
    "}\n",
    "\n",
    "def preparence(dataset: DataSet,\n",
    "                 batch_size: int):\n",
    "    dataloader_generator = DataSet_2_DataLoader_generator[dataset]\n",
    "    full_train_loader, train_loader, test_loader = dataloader_generator(batch_size)\n",
    "    full_random_train_loader, random_train_loader = get_random_dataloader(full_train_loader, batch_size)\n",
    "    \n",
    "    return full_random_train_loader, random_train_loader, test_loader\n",
    "\n",
    "full_random_train_loader, random_train_loader, test_loader = preparence(DataSet.MNIST, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_scalar_to_array(labels: torch.tensor, all_class_size: int) -> torch.Tensor:\n",
    "    outputs = []\n",
    "    for label in labels:\n",
    "        label = int(label.tolist())\n",
    "        label_full_size = [0] * all_class_size\n",
    "        label_full_size[label] = 1\n",
    "        outputs.append(label_full_size)\n",
    "    \n",
    "    outputs = torch.tensor(outputs)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uttS2WgHgpB"
   },
   "source": [
    "# Model excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "izJeh7tu-WWG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def mae_loss(outputs, labels):\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    loss_fn = nn.L1Loss(reduction='none')\n",
    "    loss_matrix = loss_fn(sm_outputs, labels.float())\n",
    "    sample_loss = loss_matrix.sum(dim=-1)\n",
    "    return sample_loss\n",
    "    \n",
    "def mse_loss(outputs, labels):\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    loss_matrix = loss_fn(sm_outputs, labels.float())\n",
    "    sample_loss = loss_matrix.sum(dim=-1)\n",
    "    return sample_loss\n",
    "\n",
    "def gce_loss(outputs, labels):\n",
    "    q = 0.7\n",
    "    sm_outputs = F.softmax(outputs, dim=1)\n",
    "    pow_outputs = torch.pow(sm_outputs, q)\n",
    "    sample_loss = (1-(pow_outputs*labels).sum(dim=1))/q # n\n",
    "    return sample_loss\n",
    "\n",
    "def phuber_ce_loss(outputs, labels):\n",
    "    trunc_point = 0.1\n",
    "    n = labels.shape[0]\n",
    "    soft_max = nn.Softmax(dim=1)\n",
    "    sm_outputs = soft_max(outputs)\n",
    "    final_outputs = sm_outputs * labels\n",
    "    final_confidence = final_outputs.sum(dim=1)\n",
    "    ce_index = (final_confidence > trunc_point)\n",
    "    sample_loss = torch.zeros(n).to(device)\n",
    "\n",
    "    if ce_index.sum() > 0:\n",
    "        ce_outputs = outputs[ce_index,:]\n",
    "        logsm = nn.LogSoftmax(dim=-1)\n",
    "        logsm_outputs = logsm(ce_outputs)\n",
    "        final_ce_outputs = logsm_outputs * labels[ce_index,:]\n",
    "        sample_loss[ce_index] = - final_ce_outputs.sum(dim=-1)\n",
    "\n",
    "    linear_index = (final_confidence <= trunc_point)\n",
    "\n",
    "    if linear_index.sum() > 0:\n",
    "        sample_loss[linear_index] = -math.log(trunc_point) + (-1/trunc_point)*final_confidence[linear_index] + 1\n",
    "\n",
    "    return sample_loss\n",
    "\n",
    "def ce_loss(outputs, labels):\n",
    "    logsm = nn.LogSoftmax(dim=1)\n",
    "    logsm_outputs = logsm(outputs)\n",
    "    final_outputs = logsm_outputs * labels\n",
    "    sample_loss = - final_outputs.sum(dim=1)\n",
    "    return sample_loss\n",
    "\n",
    "def W_O_loss(loss_fn_, outputs, labels, device, question_class_size, all_class_size):\n",
    "    n, k = labels.shape[0], labels.shape[1]\n",
    "    temp_loss = torch.zeros(n, k).to(device)\n",
    "    for i in range(k):\n",
    "        tempY = torch.zeros(n, k).to(device)\n",
    "        tempY[:, i] = 1.0\n",
    "        temp_loss[:, i] = loss_fn_(outputs, tempY)\n",
    "        \n",
    "    candidate_loss = (temp_loss * labels).sum(dim=1)\n",
    "    noncandidate_loss = (temp_loss * (1-labels)).sum(dim=1)\n",
    "    total_loss = candidate_loss - ((all_class_size - question_class_size) * (all_class_size - question_class_size - 1))/(question_class_size * (2*all_class_size - question_class_size - 1.0)) * noncandidate_loss\n",
    "    average_loss = total_loss.mean()\n",
    "    return average_loss\n",
    "\n",
    "def I_I_loss(loss_fn_, outputs, labels, device, question_class_size, all_class_size):\n",
    "    n, k = labels.shape[0], labels.shape[1]\n",
    "    temp_loss = torch.zeros(n, k).to(device)\n",
    "    for i in range(k):\n",
    "        tempY = torch.zeros(n, k).to(device)\n",
    "        tempY[:, i] = 1.0\n",
    "        temp_loss[:, i] = loss_fn_(outputs, tempY)\n",
    "        \n",
    "    candidate_loss = (temp_loss * labels).sum(dim=1)\n",
    "    noncandidate_loss = (temp_loss * (1-labels)).sum(dim=1)\n",
    "    total_loss = candidate_loss - (2*question_class_size**2 + all_class_size**2 - all_class_size*(2*question_class_size + 1))/(2*question_class_size * (all_class_size - question_class_size)) * noncandidate_loss\n",
    "    average_loss = total_loss.mean()\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7eQ0uYqy-WWH"
   },
   "outputs": [],
   "source": [
    "class mlp_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(mlp_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, self.num_flat_features(x))\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HiI0NlQ1-WWH"
   },
   "outputs": [],
   "source": [
    "def accuracy_check(loader, model, device):\n",
    "    with torch.no_grad():\n",
    "        total, num_samples = 0, 0\n",
    "        for images, labels in loader:\n",
    "            labels, images = labels.to(device), images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += (predicted == labels).sum().item()\n",
    "            num_samples += labels.size(0) \n",
    "    return total / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_check(loader, model, criterion, device, all_class_size):\n",
    "    with torch.no_grad():\n",
    "        total, num_samples = 0, 0\n",
    "        for images, labels in loader:\n",
    "            labels, images = labels.to(device), images.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = transform_scalar_to_array(labels, all_class_size)\n",
    "            labels = labels.to(device)\n",
    "            loss = criterion(outputs,labels)\n",
    "            total += loss.sum().item()\n",
    "            num_samples += labels.size(0) \n",
    "    return total / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "naI292b5-WWH"
   },
   "outputs": [],
   "source": [
    "def show_loss(epoch,max_epoch, loss):\n",
    "    print('TRAIN EPOCH[{:03}/{:03}] LOSS:{:03f}%'.format(epoch, max_epoch, loss))    \n",
    "def show_eval_loss(epoch,max_epoch, loss, is_val):\n",
    "    if is_val:\n",
    "        print('EVAL TEST EPOCH[{:03}/{:03}] LOSS:{:03f}%'.format(epoch, max_epoch, loss))\n",
    "    else:\n",
    "        print('EVAL TRAIN EPOCH[{:03}/{:03}] LOSS:{:03f}%'.format(epoch, max_epoch, loss))\n",
    "def show_acc(epoch,max_epoch, acc, is_val):\n",
    "    if is_val:\n",
    "        print('TEST EPOCH[{:03}/{:03}] ACC:{:03f}%'.format(epoch, max_epoch, acc*100))\n",
    "    else:\n",
    "        print('TRAIN EPOCH[{:03}/{:03}] ACC:{:03f}%'.format(epoch, max_epoch, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Qr7jT0aV-WWI"
   },
   "outputs": [],
   "source": [
    "def Train(model, \n",
    "          question_type, \n",
    "          question_class_size, \n",
    "          all_class_size, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          data_loader, \n",
    "          device, \n",
    "          epoch, \n",
    "          max_epoch):\n",
    "        \n",
    "    total_loss_train = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for n, (data, label) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(data)\n",
    "        if question_type == QuestionType.WHICH_ONE:\n",
    "            loss = W_O_loss(criterion, output, label.float(), device, question_class_size, all_class_size)\n",
    "        elif question_type == QuestionType.IS_IN:\n",
    "            loss = I_I_loss(criterion, output, label.float(), device, question_class_size, all_class_size)\n",
    "        else:\n",
    "            loss = criterion(output,label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "    show_loss(epoch+1, max_epoch, total_loss_train/(n+1))\n",
    "    print()\n",
    "        #show_score(epoch+1, max_epoch, n+1, len(data_loader), total_acc_test/(n+1), is_val=True)\n",
    "        #print()\n",
    "\n",
    "    return total_loss_train/(n+1), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval(model, \n",
    "        evaluation_data_loader_train, \n",
    "        evaluation_data_loader_test,\n",
    "        all_class_size,\n",
    "        criterion,\n",
    "        device, \n",
    "        epoch, \n",
    "        max_epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    #total_acc_train = accuracy_check(evaluation_data_loader_train, model, device)\n",
    "    #total_acc_test = accuracy_check(evaluation_data_loader_test, model, device)\n",
    "    total_loss_train = loss_check(evaluation_data_loader_train, model, criterion, device, all_class_size)\n",
    "    total_loss_test = loss_check(evaluation_data_loader_test, model, criterion, device, all_class_size)\n",
    "\n",
    "\n",
    "    show_eval_loss(epoch+1, max_epoch, total_loss_train, is_val=False)\n",
    "    print()\n",
    "    show_eval_loss(epoch+1, max_epoch, total_loss_test, is_val=True)\n",
    "    print()\n",
    "        \n",
    "    return total_loss_train , total_loss_test, model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model = {mlp_model: 'MLP'}\n",
    "get_loss_function = {ce_loss: 'CE_LOSS', \n",
    "                     mae_loss: 'MAE_LOSS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U3UMkIUz-WWI"
   },
   "outputs": [],
   "source": [
    "def Q_A_label_Train_Eval(dataset, \n",
    "                          question_type, \n",
    "                          question_class_size, \n",
    "                          model_name, \n",
    "                          batch_size, \n",
    "                          loss_fn, \n",
    "                          EPOCHS):\n",
    "    print(f'START: {question_type}, {question_class_size}')\n",
    "    \n",
    "    full_Q_A_labels_matrix_train_loader, Q_A_labels_matrix_train_loader, dimension, all_class_size = generate_dataloader_with_Q_A_label(full_random_train_loader,\n",
    "                                                                                                                                       question_type,\n",
    "                                                                                                                                       question_class_size,\n",
    "                                                                                                                                       batch_size)\n",
    "    full_Q_A_datas, full_Q_A_labels = next(iter(full_Q_A_labels_matrix_train_loader))\n",
    "    counter = collections.Counter(torch.sum(full_Q_A_labels, 1).int().tolist())\n",
    "    print(counter)\n",
    "    \n",
    "    \n",
    "    DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = model_name(input_dim=dimension, hidden_dim=500, output_dim=all_class_size).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    train_loss_list = []\n",
    "    eval_train_loss_list = []\n",
    "    eval_test_loss_list = []\n",
    "    result_df = pd.DataFrame(columns=[\"dataset\", \"epoch\", \"question_type\", \"question_class_size\", \"model\", \"loss_function\", \"train_loss\", \"train_acc\", \"test_acc\"])\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        train_loss, model = Train(model=model, \n",
    "                                  question_type=question_type,\n",
    "                                  question_class_size=question_class_size,\n",
    "                                  all_class_size=all_class_size,\n",
    "                                  criterion=loss_fn,\n",
    "                                  optimizer=optimizer, \n",
    "                                  data_loader=Q_A_labels_matrix_train_loader,\n",
    "                                  device=DEVICE, \n",
    "                                  epoch=epoch, \n",
    "                                  max_epoch=EPOCHS)\n",
    "        \n",
    "        eval_train_loss, eval_test_loss, model = Eval(model=model, \n",
    "                                                        evaluation_data_loader_train=random_train_loader, \n",
    "                                                        evaluation_data_loader_test=test_loader,\n",
    "                                                        all_class_size=all_class_size,\n",
    "                                                        criterion=loss_fn,\n",
    "                                                        device=DEVICE, \n",
    "                                                        epoch=epoch, \n",
    "                                                        max_epoch=EPOCHS)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        eval_train_loss_list.append(eval_train_loss)\n",
    "        eval_test_loss_list.append(eval_test_loss)\n",
    "\n",
    "        #print(f'TRAIN EPOCH[{epoch+1}/{EPOCHS}] LOSS: {np.mean(train_loss_list)} ACC: {np.mean(train_acc_list)}')\n",
    "        #print(f'TEST EPOCH[{epoch+1}/{EPOCHS}] ACC:{np.mean(test_acc_list)}')\n",
    "\n",
    "        result_df = result_df.append({'dataset': get_dataset_name[dataset],\n",
    "                                      'epoch': epoch+1,\n",
    "                                      'question_type': get_question_type[question_type], \n",
    "                                      'question_class_size': question_class_size, \n",
    "                                      'model': get_model[model_name],\n",
    "                                      'loss_function': get_loss_function[loss_fn],\n",
    "                                      'train_loss': train_loss,\n",
    "                                      'eval_train_loss': eval_train_loss,\n",
    "                                      'eval_test_loss': eval_test_loss}, ignore_index=True)\n",
    "    if question_type == QuestionType.WHICH_ONE:\n",
    "        if question_class_size != 9: \n",
    "            result_df['label_size_cand'] = counter[1]\n",
    "            result_df['label_size_comp'] = counter[all_class_size - question_class_size]\n",
    "        else:\n",
    "            result_df['label_size_cand'] = counter[1]\n",
    "            result_df['label_size_comp'] = np.nan\n",
    "    elif question_type == QuestionType.IS_IN:\n",
    "        if question_class_size != 5: \n",
    "            result_df['label_size_cand'] = counter[question_class_size]\n",
    "            result_df['label_size_comp'] = counter[all_class_size - question_class_size]\n",
    "        else:\n",
    "            result_df['label_size_cand'] = counter[question_class_size]\n",
    "            result_df['label_size_comp'] = np.nan\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "i5I4hQib-WWI",
    "outputId": "6dc1435a-248a-4f8e-92bc-121f7695d939",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983678ea8ee9457dbf83cee3c05b76e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa4fa0c58f34ec3902592e6b5cbdde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a59002612b4e43ad62b124302ebd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804fa61e2a804db2bdfcab7b428f6b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START: QuestionType.WHICH_ONE, 1\n",
      "Counter({9: 9076, 1: 924})\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7a099efc0ae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mquestion_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQuestionType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHICH_ONE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQuestionType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIS_IN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mquestion_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     result_df = Q_A_label_Train_Eval(dataset=dataset_name, \n\u001b[0m\u001b[0;32m      9\u001b[0m                                                         \u001b[0mquestion_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                                                         \u001b[0mquestion_class_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-d25b38a8143c>\u001b[0m in \u001b[0;36mQ_A_label_Train_Eval\u001b[1;34m(dataset, question_type, question_class_size, model_name, batch_size, loss_fn, EPOCHS)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mDEVICE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_class_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    605\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_name in tqdm([DataSet.MNIST, DataSet.KMNIST, DataSet.FASHION]):\n",
    "    results = []\n",
    "    full_random_train_loader, random_train_loader, test_loader = preparence(dataset_name, 512)\n",
    "    model_name = mlp_model\n",
    "    for itr in tqdm(range(1,6)):\n",
    "        for question_type in tqdm([QuestionType.WHICH_ONE, QuestionType.IS_IN]):\n",
    "            for question_size in tqdm(range(1, 10)):\n",
    "                    result_df = Q_A_label_Train_Eval(dataset=dataset_name, \n",
    "                                                        question_type=question_type, \n",
    "                                                        question_class_size=question_size, \n",
    "                                                        model_name=model_name, \n",
    "                                                        batch_size=500, \n",
    "                                                        loss_fn=mae_loss, \n",
    "                                                        EPOCHS=800)\n",
    "                    results.append(result_df)\n",
    "    output = pd.concat(results, axis=0)\n",
    "    output.to_csv(f'all_result_{get_dataset_name[dataset_name]}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
